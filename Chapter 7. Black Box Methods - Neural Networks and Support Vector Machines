###Chapte7. Black Box Methods - Neural Networks and Support Vector Machines###

##backpropagation##

concrete <- read.csv("concrete.csv")
str(concrete)
'data.frame':	1030 obs. of  9 variables:
  $ cement      : num  141 169 250 266 155 ...
$ slag        : num  212 42.2 0 114 183.4 ...
$ ash         : num  0 124.3 95.7 0 0 ...
$ water       : num  204 158 187 228 193 ...
$ superplastic: num  0 10.8 5.5 0 9.1 0 0 6.4 0 9 ...
$ coarseagg   : num  972 1081 957 932 1047 ...
$ fineagg     : num  748 796 861 670 697 ...
$ age         : int  28 14 28 28 28 90 7 56 28 28 ...
$ strength    : num  29.9 23.5 29.2 45.9 18.3 ...
#8 features and one outcome we expected

#neural network는 input data의 scale이 0에 가까운 것을 좋아하므로 input data를 normalize시켜줌.

normalize <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)

#train data와 test data 나누기
concrete_train <- concrete_norm[1:773,]
concrete_test <- concrete_norm[774:1030,]

#multilayer feedforward neural network 사용
install.packages("neuralnet")
library(neuralnet)
#model building function: m <- neuralnet(target~predictors, data=dataname, hidden=1)
# -> priediction 할 수 있는 neural network object를 반환.
# making prediction : compute(m, test)

#1개의 hidden node를 가지고 simplest multilayer feedforward network를 training시킴
concrete_model <- neuralnet(strength~cement+slag+ash+water+superplastic
                            +coarseagg+fineagg+age, data=concrete_train)
#visualize network topology
plot(concrete_model)

#evaluating model performance
model_results <- compute(concrete_model, concrete_test[,1:8])
#compute 함수는 predict와 달리 두 개의 list를 반환함.
# $neurons : network의 neuron들을 담은 list
# $net.result : pridicte value를 담음.
predicted_strength <- model_results$net.result

#numeric prediction이기 때문에 표 대신 correlation을 통해 predict value와 true value의 linear association을 파악한다.
cor(predicted_strength, concrete_test$strength)
#상관관계가 0.81정도로 꽤 강한 relationship을 갖고 있음을 알 수 있다.
set.seed(12345)

#improving model performance -> hidden node 갯수 늘리기
concrete_model2 <- neuralnet(strength ~ cement + slag + ash+water
                             +superplastic+coarseagg+fineagg+age, data=concrete_train, hidden=5)
plot(concrete_model2)
#error가 5.08에서 1.62로 줄었다. number of training step은 훨씬 늘어남.
model_results2 <- compute(concrete_model2, concrete_test[,1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
#cor가 0.81에서 0.92정도로 둘 간의 relationship이 훨씬 높아짐.




##performing Optical Character Recognition(OCR) with SVMs##
#써놓은 여러 글자들을 보고 어떤 글자인지 분류#
letters <- read.csv("letterdata.csv")
str(letters)
#SVM은 모든 변수가 numeric하고 크지 않은 scale을 가지고 있어야 한다.

#train data와 test data 나누기
letters_train <- letters[1:16000,]
letters_test <- letters[16001:20000,]

#모델 만들기
install.packages("kernlab")
library(kernlab)
#model building function: m <- ksvm(target~predictors, data=dataname, kernel="rbfdot", C=1)
#making predictions : predict(m, test, type="response")
letter_classifier <- ksvm(letter~., data=letters_train, kernel="vanilladot")
#kernel="vanilladot" : linear kernel을 이용함
letter_classifier

Support Vector Machine object of class "ksvm" 

SV type: C-svc  (classification) 
parameter : cost C = 1 

Linear (vanilla) kernel function. 

Number of Support Vectors : 7037 

#evaluating model performance
letter_predictions <- predict(letter_classifier, letters_test)

head(letter_predictions)

#분류가 잘 되었는지를 table()을 통해 확인
table(letter_predictions, letters_test$letter)

#예측이 실제랑 같으면 TRUE, 다르면 FALSE를 반환하는 함수 입력
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))

#improvin model performance : linear kernel 대신 Gaussian RBF kernel 사용하여
#더 복잡하고 higher dimensional space를 갖는 모델 만들기 : kernel="rbfdot"
letter_classifier_rbf <- ksvm(letter~., data=letters_train, kernel="rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
#true의 비율이 0.83925에서 0.93125로 높아져따!
